{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZAD 1\n",
    "Zaimplementuj przynajmniej 3 \"metryki\" spośród wymienionych: cosinusowa, LCS, DICE, euklidesowa, Levenshteina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def delta(x, y):\n",
    "    return 1 if x == y else 0\n",
    "\n",
    "\n",
    "def lcs(x, y):\n",
    "    path = [[0 for _ in range(len(y) + 1)] for _ in range(len(x) + 1)]\n",
    "    longest = 0\n",
    "\n",
    "    for i in range(1, len(x) + 1):\n",
    "        for j in range(1, len(y) + 1):\n",
    "            path[i][j] = path[i - 1][j - 1] + delta(x[i - 1], y[j - 1])\n",
    "            if path[i][j] > longest:\n",
    "                longest = path[i][j]\n",
    "\n",
    "    return 1 - longest / max(len(x), len(y))\n",
    "\n",
    "\n",
    "def get_n_grams(x, n):\n",
    "    n_grams = dict()\n",
    "    for i in range(len(x) + 1 - n):\n",
    "        n_gram = x[i:i + n]\n",
    "        if n_gram in n_grams.keys():\n",
    "            n_grams[n_gram] += 1\n",
    "        else:\n",
    "            n_grams[n_gram] = 1\n",
    "\n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def dice(x, y, n=2):\n",
    "    n_grams_x = set(get_n_grams(x, n))\n",
    "    n_grams_y = set(get_n_grams(y, n))\n",
    "    prod = n_grams_x & n_grams_y\n",
    "\n",
    "    return 1 - len(prod) * 2 / (len(n_grams_x) + len(n_grams_y))\n",
    "\n",
    "\n",
    "def levenstein(x, y):\n",
    "    edit_dist = [[0 for _ in range(len(y) + 1)] for _ in range(len(x) + 1)]\n",
    "    for i in range(1, len(x) + 1):\n",
    "        edit_dist[i][0] = i\n",
    "    for i in range(1, len(y) + 1):\n",
    "        edit_dist[0][i] = i\n",
    "    for i in range(1, len(x) + 1):\n",
    "        for j in range(1, len(y) + 1):\n",
    "            edit_dist[i][j] = min(edit_dist[i - 1][j - 1] + delta(x[i - 1], y[j - 1]),\n",
    "                                  edit_dist[i - 1][j] + 1,\n",
    "                                  edit_dist[i][j - 1] + 1)\n",
    "    return edit_dist[-1][-1] / max(len(x), len(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZAD 2\n",
    "Zaimplementuj przynajmniej 1 sposoby oceny jakości klasteryzacji (np. indeks Daviesa-Bouldina).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def DaviesBouldin(clusters, metric):\n",
    "    for cluster in clusters:\n",
    "        cluster.sort(key=lambda l1: sum([metric(l1, l2) for l2 in cluster if l1 != l2]))\n",
    "    centroids = [cluster[len(cluster) // 2] for cluster in clusters]\n",
    "\n",
    "    mean_dist = []\n",
    "    for cluster in clusters:\n",
    "        s = 0\n",
    "        n = len(cluster)\n",
    "        for l1, l2 in itertools.combinations(cluster, 2):\n",
    "            s += metric(l1, l2)\n",
    "        if n == 1:\n",
    "            mean_dist.append(0)\n",
    "        else:\n",
    "            mean_dist.append(s / (n * (n - 1) / 2))\n",
    "\n",
    "    _max = [0 for _ in range(len(clusters))]\n",
    "    for i, c1 in enumerate(clusters):\n",
    "        for j, c2 in enumerate(clusters):\n",
    "            if i != j:\n",
    "                try:\n",
    "                    a = (mean_dist[i] + mean_dist[j]) / metric(centroids[i], centroids[j])\n",
    "                except:\n",
    "                    a = (mean_dist[i] + mean_dist[j])\n",
    "                _max[i] = max(a, _max[i])\n",
    "\n",
    "    return sum(_max) / len(clusters)\n",
    "\n",
    "\n",
    "def dunn_index(clusters, metric):\n",
    "    for cluster in clusters:\n",
    "        cluster.sort(key=lambda l1: sum([metric(l1, l2) for l2 in cluster if l1 != l2]))\n",
    "    centroids = [cluster[len(cluster) // 2] for cluster in clusters]\n",
    "\n",
    "    numerator = float('inf')\n",
    "    for i, c1 in enumerate(clusters):\n",
    "        for j, c2 in enumerate(clusters):\n",
    "            if i < j:\n",
    "                numerator = min(metric(centroids[i], centroids[j]), numerator)\n",
    "    denominator = max([len(cluster) for cluster in clusters])\n",
    "\n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZAD 3\n",
    "Stwórz stoplistę najczęściej występujących słów i zastosuj ją jako pre-processing dla nazw. Algorytmy klasteryzacji powinny działać na dwóch wariantach: z pre-processingiem i bez pre-processingu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class StopList:\n",
    "    def __init__(self, text, frequency):\n",
    "        words = []\n",
    "        for line in text:\n",
    "            words += line.split()\n",
    "        counter = Counter(words)\n",
    "        self.common = {key for key, value in counter.items() if value >= frequency * len(words)}\n",
    "\n",
    "    def remove_common(self, text):\n",
    "        result = []\n",
    "        for line in text:\n",
    "            result.append(\" \".join([w for w in line.split() if w not in self.common]))\n",
    "        return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZAD 4\n",
    "Wykonaj klasteryzację zawartości załączonego pliku (lines.txt) przy użyciu  metryk zaimplementowanych w pkt. 1. Każda linia to adres pocztowy firmy, różne sposoby zapisu tego samego adresu powinny się znaleźć w jednym klastrze.\n",
    "# ZAD 5\n",
    "Porównaj jakość wyników sposobami zaimplementowanymi w pkt. 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file, n):\n",
    "    with open(file, \"r\", encoding=\"UTF-8\") as f:\n",
    "        text = f.read().splitlines()\n",
    "    return text[:n]\n",
    "\n",
    "def cluster_lines(lines, metric, threshold):\n",
    "    clusters = []\n",
    "    for line in lines:\n",
    "        found_cluster = False\n",
    "        for cluster in clusters:\n",
    "            if any(metric(line, cluster_line) <= threshold for cluster_line in cluster):\n",
    "                cluster.append(line)\n",
    "                found_cluster = True\n",
    "                break\n",
    "        if not found_cluster:\n",
    "            clusters.append([line])\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering using LCS:\n",
      "Clustering using Dice coefficient:\n",
      "Clustering using Levenshtein distance:\n"
     ]
    }
   ],
   "source": [
    "lines = read_text(\"lines.txt\", 200)\n",
    "print(\"Clustering using LCS:\")\n",
    "lcs_clusters = cluster_lines(lines, lcs, 0.8)\n",
    "with open(\"lcs_clusters.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    for i, cluster in enumerate(lcs_clusters):\n",
    "        f.write(f\"Cluster {i+1}: {cluster}\\n\")\n",
    "\n",
    "\n",
    "print(\"Clustering using Dice coefficient:\")\n",
    "dice_clusters = cluster_lines(lines, dice, 0.8)\n",
    "with open(\"dice_clusters.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    for i, cluster in enumerate(dice_clusters):\n",
    "        f.write(f\"Cluster {i+1}: {cluster}\\n\")\n",
    "\n",
    "\n",
    "print(\"Clustering using Levenshtein distance:\")\n",
    "lev_clusters = cluster_lines(lines, levenstein, 0.6)\n",
    "with open(\"lev_clusters.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    for i, cluster in enumerate(lev_clusters):\n",
    "        f.write(f\"Cluster {i+1}: {cluster}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davies-Bouldin index for LCS clusters: 1.1894017735187954\n",
      "Davies-Bouldin index for Dice coefficient clusters: 0.9421675144477121\n",
      "Davies-Bouldin index for Levenshtein distance clusters: 0.32822690520049685\n"
     ]
    }
   ],
   "source": [
    "print(\"Davies-Bouldin index for LCS clusters:\", DaviesBouldin(lcs_clusters, lcs))\n",
    "print(\"Davies-Bouldin index for Dice coefficient clusters:\", DaviesBouldin(dice_clusters, dice))\n",
    "print(\"Davies-Bouldin index for Levenshtein distance clusters:\", DaviesBouldin(lev_clusters, levenstein))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering using LCS:\n",
      "Clustering using Dice coefficient:\n",
      "Clustering using Levenshtein distance:\n"
     ]
    }
   ],
   "source": [
    "lines = read_text(\"lines.txt\", 200)\n",
    "stoplist = StopList(lines, 0.05)  # create stoplist with words that appear in at least 5% of the lines\n",
    "lines_without_common = stoplist.remove_common(lines)  # remove common words from the lines\n",
    "\n",
    "lines = lines_without_common\n",
    "\n",
    "print(\"Clustering using LCS:\")\n",
    "lcs_clusters = cluster_lines(lines, lcs, 0.8)\n",
    "with open(\"stoplist_lcs_clusters.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    for i, cluster in enumerate(lcs_clusters):\n",
    "        f.write(f\"Cluster {i+1}: {cluster}\\n\")\n",
    "\n",
    "print(\"Clustering using Dice coefficient:\")\n",
    "dice_clusters = cluster_lines(lines, dice, 0.8)\n",
    "with open(\"stoplist_dice_clusters.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    for i, cluster in enumerate(dice_clusters):\n",
    "        f.write(f\"Cluster {i+1}: {cluster}\\n\")\n",
    "\n",
    "print(\"Clustering using Levenshtein distance:\")\n",
    "lev_clusters = cluster_lines(lines, levenstein, 0.6)\n",
    "with open(\"stoplist_lev_clusters.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    for i, cluster in enumerate(lev_clusters):\n",
    "        f.write(f\"Cluster {i+1}: {cluster}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davies-Bouldin index for LCS clusters with stoplist: 1.1656477064993143\n",
      "Davies-Bouldin index for Dice coefficient clusters with stoplist: 0.9423491875636503\n",
      "Davies-Bouldin index for Levenshtein distance clusters with stoplist: 0.3267960284204245\n"
     ]
    }
   ],
   "source": [
    "print(\"Davies-Bouldin index for LCS clusters with stoplist:\", DaviesBouldin(lcs_clusters, lcs))\n",
    "print(\"Davies-Bouldin index for Dice coefficient clusters with stoplist:\", DaviesBouldin(dice_clusters, dice))\n",
    "print(\"Davies-Bouldin index for Levenshtein distance clusters with stoplist:\", DaviesBouldin(lev_clusters, levenstein))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZAD 6\n",
    "Czy masz jakiś pomysł na poprawę jakości klasteryzacji w tym zadaniu?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
